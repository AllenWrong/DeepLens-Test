{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccab046-099e-40b1-a5de-94ae0312e22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7494b942-17a7-4ef8-99ad-e7a3bde081f6",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8de412-58f4-48d0-a61f-2fd4a21a4ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.backends import cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4b0dd6-1a51-4d5c-bc97-685a79f93c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Because some mass is a very small value. In order to observe the loss on those small value,\n",
    "# we set the precision we cared.\n",
    "cared_precision = 0.00001\n",
    "\n",
    "# set seed and set deterministic behavior, which ensure the reproducible\n",
    "SEED = 2022\n",
    "torch.cuda.manual_seed_all(SEED) if torch.cuda.is_available() else torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f70fe-f34f-4d9a-9ff3-8c785a7d338c",
   "metadata": {},
   "source": [
    "# Data Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa00a380-9ef6-428a-a55f-4086e7a11fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d24cc-a9e3-41c6-8926-36b46b63eb51",
   "metadata": {},
   "source": [
    "## Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e37575-2889-4fe5-8060-4d44dad93608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, data_dir, transform=None):\n",
    "        self.csv_file = pd.read_csv(csv_file)\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        print(self.transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_dir = os.path.join(self.data_dir, self.csv_file.iloc[item, 0])\n",
    "        img = np.load(img_dir, allow_pickle=True)\n",
    "\n",
    "        # MaxMin normalization.\n",
    "        x = (img[0] - img[0].min()) / (img[0].max() - img[0].min())\n",
    "\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, torch.tensor(img[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fc793-335b-461f-afe4-9522e064a515",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f932e0-bb2f-4999-a677-c39153cc05ec",
   "metadata": {},
   "source": [
    "I have tried batch_size 64, batch_size 128 and batch_size 200 and found that batch_size 100 may be a better choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d236a8-8e52-415d-bbc1-222f33ce5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    lr = 1e-4\n",
    "    batch_size = 100\n",
    "    num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de980bab-63f5-45d2-88d6-377dfc14b930",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554618b4-6d72-414a-85e1-ab199f90de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.nn import Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d978188b-f5e7-439c-82bc-8dd5ebdd3de2",
   "metadata": {},
   "source": [
    "The model architecture is simple. Two full connection layer with ReLU and Sigmoid activation function are appended to the tail of ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "877b252b-c58a-4eab-aacc-1921062d0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetReg(nn.Module):\n",
    "\n",
    "    def __init__(self, pre_trained):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pre_trained: True if want to use pretrained weight else false\n",
    "        \"\"\"\n",
    "        super(ResNetReg, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=pre_trained)\n",
    "        self.reg = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.backbone.conv1 = Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.backbone.fc = self.reg\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda587d-4af2-4296-8aa5-c5b55036e836",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ce9a15-e6d5-4288-b1fb-48c15a4ccea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim  # optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms  # many transforms used to data set\n",
    "from tqdm import tqdm\n",
    "from utils import check_accuracy, save_checkpoint, create_description, diff_rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbaac16-13c2-43bf-a74b-782d2af3088e",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670e573d-81b3-43fb-a3f7-86074214cedc",
   "metadata": {},
   "source": [
    "- **lr**: Learning rate is initialized with 1e-4. And it will be decayed with a factor of 0.8 if the val loss does not decrease in 3 epochs.\n",
    "\n",
    "- **cared_precision**: May be you have noticed that the value of the val mse and train mse is bigger than one. I noticed that y is a decimal much smaller than 1. So the mse is also small. In order to observe the change of MSE more clearly, I scaled the value of MSE. If you want to use the original value, you can set the cared_precision with value 1.\n",
    "\n",
    "- **model save**: After 20 epoch, I will choose a model with the min validation mse to save.\n",
    "\n",
    "- **r2**: just a reference criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16212edb-8785-417d-a605-370b7cf013b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(cfg, train_loader, val_loader, model, optimizer, criterion, flatten=False):\n",
    "    \"\"\"\n",
    "    train function\n",
    "    \"\"\"\n",
    "\n",
    "    min_loss = 0\n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        losses_per_batch = []\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=0.8,\n",
    "            patience=3,\n",
    "            verbose=True,\n",
    "            min_lr=1e-5\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        p_bar = tqdm(enumerate(train_loader), leave=False, total=len(train_loader), file=sys.stdout)\n",
    "        for batch_idx, (x_batch, y_batch) in p_bar:\n",
    "            # move data to gpu if cuda is available\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            if flatten:\n",
    "                x_batch = x_batch.reshape((x_batch.shape[0], -1))\n",
    "\n",
    "            # forward\n",
    "            scores = model.forward(x_batch)\n",
    "\n",
    "            loss = criterion(scores.ravel(), y_batch)\n",
    "            losses_per_batch.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # used to compute diff rate\n",
    "            y_true_list.extend(list(y_batch.cpu().numpy()))\n",
    "            y_pred_list.extend(list(scores.detach().ravel().cpu().numpy()))\n",
    "\n",
    "        # compute acc and loss\n",
    "        val_loss, r2, _ = check_accuracy(val_loader, model, criterion, device, flatten)\n",
    "        train_loss = sum(losses_per_batch) / len(losses_per_batch)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # checkpoint\n",
    "        if epoch == 20:\n",
    "            min_loss = val_loss\n",
    "\n",
    "        if epoch > 20 and val_loss < min_loss:\n",
    "            check_point = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict()\n",
    "            }\n",
    "            print(f\"epoch {epoch+1}: \", end=\"\")\n",
    "            save_checkpoint(check_point, \"./checkpoint/lowest_loss_model.pth.tar\")\n",
    "            min_loss = val_loss\n",
    "\n",
    "        # train_diff_rate = diff_rate(y_pred_list, y_true_list)\n",
    "        end_time = time.time()\n",
    "        print(f\"epoch [{epoch + 1}/{cfg.num_epochs}], \"\n",
    "              f\"train mse: {train_loss / cared_precision:.8f}, \"\n",
    "              f\"val mse: {val_loss / cared_precision:.8f}, \"\n",
    "              f\"val r2 score: {r2:.4f}, \"\n",
    "              # f\"train diff rate: {train_diff_rate:.4f}, \"\n",
    "              # f\"val diff rate: {rate:.4f}, \"\n",
    "              f\"time used: {(end_time - start_time)/60:.4f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778180e2-2e14-4cf4-8b36-4cc789806996",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152db059-8e9d-424d-80ce-54664e9d271e",
   "metadata": {},
   "source": [
    "### Data prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9362d2d-a677-49b1-bea6-950359f027aa",
   "metadata": {},
   "source": [
    "**Before run this part, please download data from <a href=\"https://drive.google.com/file/d/1hu472ALwGPBcTCXSAM0VoCWmTktg9j-j/view\">here</a>**.\n",
    "\n",
    "After you download data, the file should be organized as following:\n",
    "\n",
    "```\n",
    "|--- Test III\n",
    "    |--- checkpoint\n",
    "    |--- lens_data\n",
    "        |--- xxx.npy\n",
    "        |--- xxx.npy\n",
    "        ...\n",
    "    |--- test description.csv\n",
    "    |--- Test III.ipynb\n",
    "    |--- train description.csv\n",
    "    |--- utils.py\n",
    "    |--- val description.csv\n",
    "    |--- y.npy\n",
    "```\n",
    "\n",
    "**The file `test description.csv`, `train description.csv` and `val description.csv` which i have provided. And you do not need to run the code `create_description(data_dir, [0.8, 0.1])`. But if you want to create new `xxx description.csv` file, you can open the comment.**\n",
    "\n",
    "**The argument [0.8, 0.1] represents the trainset_rate 0.8 and valset_rate 0.1. The testset_rate will be induced.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c4cd8e-b858-4407-8e13-aeb78955f989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor()\n",
      "    RandomAffine(degrees=[0.0, 180.0], translate=(0.2, 0.2))\n",
      "    Resize(size=(150, 150), interpolation=bilinear, max_size=None, antialias=None)\n",
      ")\n",
      "Compose(\n",
      "    ToTensor()\n",
      "    RandomAffine(degrees=[0.0, 180.0], translate=(0.2, 0.2))\n",
      "    Resize(size=(150, 150), interpolation=bilinear, max_size=None, antialias=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./lens_data\"\n",
    "# create_description(data_dir, [0.8, 0.1])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomAffine(degrees=(0, 180), translate=(0.2, 0.2)),\n",
    "    transforms.Resize((150, 150))\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "# get data set\n",
    "train_set = MyData(\"./train description.csv\", data_dir, train_transform)\n",
    "val_set = MyData(\"./val description.csv\", data_dir, val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c732732-6b84-42e9-8fef-2ab5f49e6af7",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9590fd70-dbbb-48ec-afeb-11f5e6351332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], train mse: 488.92054504, val mse: 23.23353733, val r2 score: -0.1010, time used: 4.5347m\n",
      "epoch [2/100], train mse: 23.35984664, val mse: 21.87382848, val r2 score: -0.0365, time used: 4.4552m\n",
      "epoch [3/100], train mse: 22.28330613, val mse: 21.72169860, val r2 score: -0.0293, time used: 4.4615m\n",
      "epoch [4/100], train mse: 21.86350809, val mse: 21.74953078, val r2 score: -0.0307, time used: 4.4202m\n",
      "epoch [5/100], train mse: 21.45100737, val mse: 21.28794895, val r2 score: -0.0088, time used: 4.3763m\n",
      "epoch [6/100], train mse: 21.34558314, val mse: 20.66337529, val r2 score: 0.0208, time used: 4.4063m\n",
      "epoch [7/100], train mse: 20.75944963, val mse: 20.37258441, val r2 score: 0.0346, time used: 4.4357m\n",
      "epoch [8/100], train mse: 20.45769606, val mse: 20.81315133, val r2 score: 0.0137, time used: 4.4096m\n",
      "epoch [9/100], train mse: 20.41810515, val mse: 20.07014532, val r2 score: 0.0489, time used: 4.3871m\n",
      "epoch [10/100], train mse: 19.99689106, val mse: 19.57481426, val r2 score: 0.0724, time used: 4.3664m\n",
      "epoch [11/100], train mse: 19.54792550, val mse: 19.36108601, val r2 score: 0.0825, time used: 4.3886m\n",
      "epoch [12/100], train mse: 19.07244389, val mse: 19.84361160, val r2 score: 0.0597, time used: 4.3945m\n",
      "epoch [13/100], train mse: 18.77993990, val mse: 18.85679427, val r2 score: 0.1064, time used: 4.3963m\n",
      "epoch [14/100], train mse: 18.56336064, val mse: 19.88929045, val r2 score: 0.0575, time used: 4.4219m\n",
      "epoch [15/100], train mse: 17.85031350, val mse: 18.53941562, val r2 score: 0.1215, time used: 4.4445m\n",
      "epoch [16/100], train mse: 17.64129241, val mse: 18.11509157, val r2 score: 0.1416, time used: 4.3847m\n",
      "epoch [17/100], train mse: 17.45250880, val mse: 16.67249963, val r2 score: 0.2099, time used: 4.3962m\n",
      "epoch [18/100], train mse: 16.95477785, val mse: 16.62348257, val r2 score: 0.2123, time used: 4.3860m\n",
      "epoch [19/100], train mse: 16.68532093, val mse: 16.29387695, val r2 score: 0.2279, time used: 4.3601m\n",
      "epoch [20/100], train mse: 16.10898612, val mse: 16.25043315, val r2 score: 0.2299, time used: 4.4129m\n",
      "epoch [21/100], train mse: 15.63783510, val mse: 15.69349386, val r2 score: 0.2563, time used: 4.4140m\n",
      "epoch 22: ==> Saving checkpoint                  \n",
      "epoch [22/100], train mse: 15.28534910, val mse: 14.60412059, val r2 score: 0.3079, time used: 4.3992m\n",
      "epoch 23: ==> Saving checkpoint                  \n",
      "epoch [23/100], train mse: 14.93383215, val mse: 14.34701966, val r2 score: 0.3201, time used: 4.4120m\n",
      "epoch [24/100], train mse: 14.54703923, val mse: 14.54848659, val r2 score: 0.3106, time used: 4.3751m\n",
      "epoch [25/100], train mse: 14.16455415, val mse: 14.65883203, val r2 score: 0.3054, time used: 4.4260m\n",
      "epoch 26: ==> Saving checkpoint                  \n",
      "epoch [26/100], train mse: 13.64925663, val mse: 13.04583360, val r2 score: 0.3818, time used: 4.4018m\n",
      "epoch 27: ==> Saving checkpoint                  \n",
      "epoch [27/100], train mse: 13.21485579, val mse: 12.86943270, val r2 score: 0.3901, time used: 4.3920m\n",
      "epoch [28/100], train mse: 12.82537233, val mse: 13.20144141, val r2 score: 0.3744, time used: 4.4020m\n",
      "epoch [29/100], train mse: 12.36242440, val mse: 13.53219664, val r2 score: 0.3587, time used: 4.4130m\n",
      "epoch [30/100], train mse: 12.21344271, val mse: 13.08081055, val r2 score: 0.3801, time used: 4.3734m\n",
      "epoch 31: ==> Saving checkpoint                  \n",
      "epoch [31/100], train mse: 12.01564079, val mse: 11.96236612, val r2 score: 0.4331, time used: 4.3984m\n",
      "epoch 32: ==> Saving checkpoint                  \n",
      "epoch [32/100], train mse: 11.61942176, val mse: 11.62917996, val r2 score: 0.4489, time used: 4.3906m\n",
      "epoch [33/100], train mse: 11.37729761, val mse: 25.19493550, val r2 score: -0.1939, time used: 4.3967m\n",
      "epoch 34: ==> Saving checkpoint                  \n",
      "epoch [34/100], train mse: 11.17423197, val mse: 11.47252574, val r2 score: 0.4563, time used: 4.4186m\n",
      "epoch 35: ==> Saving checkpoint                  \n",
      "epoch [35/100], train mse: 10.98852999, val mse: 11.11410713, val r2 score: 0.4733, time used: 4.4527m\n",
      "epoch 36: ==> Saving checkpoint                  \n",
      "epoch [36/100], train mse: 10.52354782, val mse: 10.75359103, val r2 score: 0.4904, time used: 4.4234m\n",
      "epoch 37: ==> Saving checkpoint                  \n",
      "epoch [37/100], train mse: 10.52649167, val mse: 9.92805770, val r2 score: 0.5295, time used: 4.4022m\n",
      "epoch [38/100], train mse: 10.07154325, val mse: 10.34296884, val r2 score: 0.5099, time used: 4.3941m\n",
      "epoch [39/100], train mse: 9.86378840, val mse: 14.48295338, val r2 score: 0.3137, time used: 4.3851m\n",
      "epoch [40/100], train mse: 10.09205765, val mse: 15.26210156, val r2 score: 0.2768, time used: 4.3955m\n",
      "epoch 41: ==> Saving checkpoint                  \n",
      "epoch [41/100], train mse: 9.71000144, val mse: 9.65994502, val r2 score: 0.5422, time used: 4.3553m\n",
      "epoch [42/100], train mse: 9.58717756, val mse: 11.57809102, val r2 score: 0.4513, time used: 4.3727m\n",
      "epoch [43/100], train mse: 9.39926988, val mse: 10.15505815, val r2 score: 0.5188, time used: 4.3484m\n",
      "epoch [44/100], train mse: 9.42496216, val mse: 15.22823040, val r2 score: 0.2784, time used: 4.3654m\n",
      "epoch 45: ==> Saving checkpoint                  \n",
      "epoch [45/100], train mse: 9.21499463, val mse: 9.43048612, val r2 score: 0.5531, time used: 4.4156m\n",
      "epoch [46/100], train mse: 9.14625882, val mse: 10.00757515, val r2 score: 0.5258, time used: 4.4004m\n",
      "epoch [47/100], train mse: 9.02216637, val mse: 10.05820657, val r2 score: 0.5234, time used: 4.4032m\n",
      "epoch [48/100], train mse: 9.12285027, val mse: 10.52054322, val r2 score: 0.5015, time used: 4.3439m\n",
      "epoch 49: ==> Saving checkpoint                  \n",
      "epoch [49/100], train mse: 8.76442857, val mse: 8.86937137, val r2 score: 0.5797, time used: 4.3636m\n",
      "epoch 50: ==> Saving checkpoint                  \n",
      "epoch [50/100], train mse: 8.84229264, val mse: 8.59626418, val r2 score: 0.5926, time used: 4.3696m\n",
      "epoch [51/100], train mse: 8.72738390, val mse: 8.85747114, val r2 score: 0.5803, time used: 4.3539m\n",
      "epoch [52/100], train mse: 8.76707361, val mse: 9.85251656, val r2 score: 0.5331, time used: 4.3611m\n",
      "epoch [53/100], train mse: 8.63138298, val mse: 8.75841471, val r2 score: 0.5850, time used: 4.3540m\n",
      "epoch [54/100], train mse: 8.51368566, val mse: 9.20328281, val r2 score: 0.5639, time used: 4.3605m\n",
      "epoch [55/100], train mse: 8.55072456, val mse: 10.23081364, val r2 score: 0.5152, time used: 4.3674m\n",
      "epoch 56: ==> Saving checkpoint                  \n",
      "epoch [56/100], train mse: 8.20194535, val mse: 8.22997678, val r2 score: 0.6100, time used: 4.3744m\n",
      "epoch [57/100], train mse: 7.98737505, val mse: 9.42099120, val r2 score: 0.5536, time used: 4.3426m\n",
      "epoch [58/100], train mse: 8.06884796, val mse: 8.33516026, val r2 score: 0.6050, time used: 4.3268m\n",
      "epoch [59/100], train mse: 8.22242806, val mse: 8.24522186, val r2 score: 0.6093, time used: 4.3198m\n",
      "epoch [60/100], train mse: 7.92679191, val mse: 9.28376182, val r2 score: 0.5601, time used: 4.3272m\n",
      "epoch 61: ==> Saving checkpoint                  \n",
      "epoch [61/100], train mse: 7.88081027, val mse: 8.06778920, val r2 score: 0.6177, time used: 4.3364m\n",
      "epoch 62: ==> Saving checkpoint                  \n",
      "epoch [62/100], train mse: 8.14187055, val mse: 7.77488414, val r2 score: 0.6316, time used: 4.3480m\n",
      "epoch [63/100], train mse: 7.58292055, val mse: 8.00204214, val r2 score: 0.6208, time used: 4.3412m\n",
      "epoch 64: ==> Saving checkpoint                  \n",
      "epoch [64/100], train mse: 7.74263387, val mse: 7.76226994, val r2 score: 0.6322, time used: 4.3482m\n",
      "epoch [65/100], train mse: 7.73417185, val mse: 7.76536994, val r2 score: 0.6320, time used: 4.3277m\n",
      "epoch [66/100], train mse: 7.62765323, val mse: 11.25511557, val r2 score: 0.4666, time used: 4.3129m\n",
      "epoch 67: ==> Saving checkpoint                  \n",
      "epoch [67/100], train mse: 7.61080134, val mse: 7.44597813, val r2 score: 0.6472, time used: 4.3550m\n",
      "epoch [68/100], train mse: 7.48950005, val mse: 7.93490570, val r2 score: 0.6240, time used: 4.3145m\n",
      "epoch [69/100], train mse: 7.65251496, val mse: 8.30549366, val r2 score: 0.6064, time used: 4.3084m\n",
      "epoch [70/100], train mse: 7.45537301, val mse: 7.73819725, val r2 score: 0.6333, time used: 4.3348m\n",
      "epoch [71/100], train mse: 7.34072994, val mse: 8.52737660, val r2 score: 0.5959, time used: 4.3332m\n",
      "epoch [72/100], train mse: 7.52714106, val mse: 9.89566482, val r2 score: 0.5311, time used: 4.3363m\n",
      "epoch [73/100], train mse: 7.22931642, val mse: 9.15270131, val r2 score: 0.5663, time used: 4.3349m\n",
      "epoch [74/100], train mse: 7.16637694, val mse: 8.15904049, val r2 score: 0.6134, time used: 4.3262m\n",
      "epoch [75/100], train mse: 7.24957697, val mse: 7.62258213, val r2 score: 0.6388, time used: 4.3355m\n",
      "epoch [76/100], train mse: 7.15091756, val mse: 8.35710651, val r2 score: 0.6040, time used: 4.3201m\n",
      "epoch [77/100], train mse: 7.11130206, val mse: 7.49262033, val r2 score: 0.6449, time used: 4.3215m\n",
      "epoch 78: ==> Saving checkpoint                  \n",
      "epoch [78/100], train mse: 7.18721049, val mse: 7.34604416, val r2 score: 0.6519, time used: 4.3577m\n",
      "epoch [79/100], train mse: 7.01134445, val mse: 8.92007443, val r2 score: 0.5773, time used: 4.3279m\n",
      "epoch [80/100], train mse: 7.10469645, val mse: 10.04982243, val r2 score: 0.5238, time used: 4.3321m\n",
      "epoch [81/100], train mse: 7.08262941, val mse: 10.15832028, val r2 score: 0.5186, time used: 4.3439m\n",
      "epoch [82/100], train mse: 6.95401800, val mse: 8.64222630, val r2 score: 0.5905, time used: 4.3270m\n",
      "epoch [83/100], train mse: 6.99411209, val mse: 8.47416459, val r2 score: 0.5984, time used: 4.3525m\n",
      "epoch [84/100], train mse: 6.82779670, val mse: 7.36666276, val r2 score: 0.6509, time used: 4.3365m\n",
      "epoch [85/100], train mse: 6.73932128, val mse: 7.43654174, val r2 score: 0.6476, time used: 4.3266m\n",
      "epoch [86/100], train mse: 6.86521464, val mse: 9.47960205, val r2 score: 0.5508, time used: 4.3362m\n",
      "epoch 87: ==> Saving checkpoint                  \n",
      "epoch [87/100], train mse: 6.72210712, val mse: 6.80759041, val r2 score: 0.6774, time used: 4.3487m\n",
      "epoch [88/100], train mse: 6.77486862, val mse: 7.23257792, val r2 score: 0.6573, time used: 4.3333m\n",
      "epoch [89/100], train mse: 6.65477141, val mse: 7.07326962, val r2 score: 0.6648, time used: 4.3290m\n",
      "epoch [90/100], train mse: 6.62732098, val mse: 8.66493600, val r2 score: 0.5894, time used: 4.3292m\n",
      "epoch [91/100], train mse: 6.71444605, val mse: 8.62394892, val r2 score: 0.5913, time used: 4.3334m\n",
      "epoch [92/100], train mse: 6.57304131, val mse: 7.06832625, val r2 score: 0.6650, time used: 4.3351m\n",
      "epoch [93/100], train mse: 6.55925451, val mse: 7.37027077, val r2 score: 0.6507, time used: 4.3315m\n",
      "epoch [94/100], train mse: 6.45382815, val mse: 7.07180728, val r2 score: 0.6649, time used: 4.3206m\n",
      "epoch [95/100], train mse: 6.46803094, val mse: 7.04260213, val r2 score: 0.6663, time used: 4.3365m\n",
      "epoch [96/100], train mse: 6.47861877, val mse: 8.44921239, val r2 score: 0.5996, time used: 4.3236m\n",
      "epoch [97/100], train mse: 6.32405395, val mse: 7.20363150, val r2 score: 0.6586, time used: 4.3111m\n",
      "epoch [98/100], train mse: 6.44942337, val mse: 10.16783859, val r2 score: 0.5182, time used: 4.3054m\n",
      "epoch [99/100], train mse: 6.40424333, val mse: 11.97386590, val r2 score: 0.4326, time used: 4.3144m\n",
      "epoch [100/100], train mse: 6.33892901, val mse: 9.54088031, val r2 score: 0.5479, time used: 4.3219m\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=cfg.batch_size)\n",
    "\n",
    "# define model, loss function and optimizer.\n",
    "model = ResNetReg(pre_trained=True).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "train_fn(cfg, train_loader, val_loader, model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9bf3eb-16bf-49c8-92af-994c05018d1a",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a9367-abd3-4cfa-9638-7246bdb352e2",
   "metadata": {},
   "source": [
    "## simple baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52a3b7f-9f05-49cd-b3cd-8c7c3f7d698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13113f7e-cb3c-4914-a564-1cdb93ca9482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 21.34528930, r2 score: 0.0000, diff rate: 0.2795.\n"
     ]
    }
   ],
   "source": [
    "def baseline():\n",
    "    y = np.load(\"y.npy\")\n",
    "    y_mean = [float(y.mean())] * len(y)\n",
    "    print(f\"mse: {((y - y.mean())**2).mean()/cared_precision:.8f}, \", end=\"\")\n",
    "    print(f\"r2 score: {r2_score(list(y), y_mean):.4f}, \", end=\"\")\n",
    "    print(f\"diff rate: {diff_rate(list(y), y_mean):.4f}.\")\n",
    "\n",
    "baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d366e2-3043-41c2-8cd4-dbdd18f21577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms  # many transforms used to data set\n",
    "from utils import load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ea42a-ea1b-49a6-89d7-52df7287b42e",
   "metadata": {},
   "source": [
    "**If you want to use my pretrained model, you can download <a href=\"https://drive.google.com/file/d/1FF7Zp6OjJVVq4hyp7wbdueRg6XwSaKL_/view?usp=sharing\">here</a>. After you download it, put it into the './checkpoint'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e80a06a4-9a8b-48e7-b978-d46c5e56cbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor()\n",
      "    RandomAffine(degrees=[0.0, 180.0], translate=(0.2, 0.2))\n",
      "    Resize(size=(150, 150), interpolation=bilinear, max_size=None, antialias=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = \"./checkpoint/lowest_loss_model.pth.tar\"\n",
    "data_dir = \"./lens_data\"\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((150, 150))\n",
    "])\n",
    "test_set = MyData(\"./test description.csv\", data_dir, val_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=cfg.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "971f467b-c50e-40be-b66f-026908c06378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "model = ResNetReg(pre_trained=False).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "load_checkpoint(checkpoint_file, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99d1732d-6d2d-4bf2-b712-cf8284b03ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379fa7f-cafb-4609-a5eb-004f8233fc9e",
   "metadata": {},
   "source": [
    "**If you do not want to use the cared_precision, you can remove it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b39cb908-ff7d-4ec0-97f2-198a7251ed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mse loss: 7.306139547242726, r2: 0.6810318628581902\n",
      "test mse loss: 7.2235091088115615, r2: 0.6846393051807977\n",
      "test mse loss: 6.890963728087273, r2: 0.6991574210638662\n",
      "test mse loss: 6.9443447187824, r2: 0.6968269379064189\n",
      "test mse loss: 7.223070315506167, r2: 0.6846584618205398\n",
      "CPU times: user 2min 56s, sys: 1.76 s, total: 2min 58s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_loss_list = []\n",
    "for i in range(test_time):\n",
    "    test_loss, r2, _ = check_accuracy(test_loader, model, criterion, device)\n",
    "    test_loss_list.append(test_loss)\n",
    "    print(f\"test mse loss: {test_loss/cared_precision}, r2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7975c05e-fbb2-41bb-88a8-aafeadaaf0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 7.1176054836860265, std: 0.16689474510760155\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean: {np.mean(test_loss_list)/cared_precision}, std: {np.std(test_loss_list)/cared_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ced5a-5f8d-4359-8fae-8cd6b4bd0e17",
   "metadata": {},
   "source": [
    "**So here the mse on the test set is about 7.11761e-5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaf5077-a7af-4652-91bd-e94468467a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
